{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2 Unnamed: 2  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
      "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
      "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
      "\n",
      "  Unnamed: 3 Unnamed: 4  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n"
     ]
    }
   ],
   "source": [
    "# Deep Learning\n",
    "# Let's build a spam clasifier\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values with empty strings\n",
    "dataset.fillna(value='', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v1' 'v2' 'Unnamed: 2' 'Unnamed: 3' 'Unnamed: 4']\n"
     ]
    }
   ],
   "source": [
    "# score phrase and title\n",
    "print(dataset.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the required columns for inputs and outputs\n",
    "totalX = dataset.v2 # SMS text\n",
    "totalY = dataset.v1 # spam or ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn\n",
    "from tflearn.data_utils import pad_sequences, VocabularyProcessor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Convert the strings in the input into integers corresponding to the dictionary positions\n",
    "# Data is automatically padded so we need to pad_sequences manually\n",
    "vocab_proc = VocabularyProcessor(30) # max document length\n",
    "totalX = np.array(list(vocab_proc.fit_transform(totalX)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,     2,     3, ...,     0,     0,     0],\n",
       "       [   21,    22,    23, ...,     0,     0,     0],\n",
       "       [   27,    28,     8, ...,    49,    50,    51],\n",
       "       ..., \n",
       "       [11153,   415,     8, ...,     0,     0,     0],\n",
       "       [  174,  3484,   286, ...,     0,     0,     0],\n",
       "       [ 4643,   660,  1641, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will have 2 classes in total for prediction, indices from 0 to 1\n",
    "vocab_proc2 = VocabularyProcessor(1)\n",
    "totalY = np.array(list(vocab_proc2.fit_transform(totalY))) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       ..., \n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       ..., \n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# here totalY is numbered dictionary entries (0, 1, ... to 10)\n",
    "totalY[5]\n",
    "# Convert the indices into 11 dimensional vectors\n",
    "totalY = to_categorical(totalY, 2)\n",
    "# here totalY is a binary matrix\n",
    "totalY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019 3741 1053  184  182  384 1754  168  590    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "[2678  185  538  105 1149    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "[ 1.  0.]\n",
      "[ 1.  0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miquel/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Split into training and testing data\n",
    "trainX, testX, trainY, testY = train_test_split(totalX, totalY, test_size=0.1)\n",
    "\n",
    "print(trainX[0])\n",
    "print(testX[0])\n",
    "print(trainY[0])\n",
    "print(testY[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Network building\n",
    "# 15 words max, so 15 input data\n",
    "net = tflearn.input_data([None, 30])\n",
    "\n",
    "# dictionary has 20k words max\n",
    "# Turns positive integers (indexes) into dense vectors of fixed size.\n",
    "net = tflearn.embedding(net, input_dim=20000, output_dim=128)\n",
    "\n",
    "# Long Short Term Memory Recurrent Layer.\n",
    "# Each input would have a size of i15x128 and each of these 128\n",
    "# sized vectors are fed into the LSTM layer one at a time.\n",
    "# All the intermediate outputs are collected and then passed on to the second LSTM layer.\n",
    "net = tflearn.lstm(net, 128, dropout=0.8)\n",
    "\n",
    "# The output is then sent to a fully connected layer that would give us our final 2 classes\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "\n",
    "# We use the adam optimizer instead of standard SGD since it converges much faster\n",
    "net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1569  | total loss: \u001b[1m\u001b[32m0.00030\u001b[0m\u001b[0m | time: 12.726s\n",
      "| Adam | epoch: 010 | loss: 0.00030 - acc: 1.0000 -- iter: 4992/5014\n",
      "Training Step: 1570  | total loss: \u001b[1m\u001b[32m0.00029\u001b[0m\u001b[0m | time: 13.809s\n",
      "| Adam | epoch: 010 | loss: 0.00029 - acc: 1.0000 | val_loss: 0.10926 - val_acc: 0.9803 -- iter: 5014/5014\n",
      "--\n",
      "INFO:tensorflow:/Users/miquel/dev/github/deep-learning/message_clasifier/spam_model.tfl is not in all_model_checkpoint_paths. Manually adding it.\n",
      "done training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "\n",
    "# uncomment to load saved model\n",
    "# model.load('spam_model.tfl')\n",
    "\n",
    "# comment to use saved model\n",
    "model.fit(trainX, trainY, validation_set=(testX, testY), show_metric=True, batch_size=32)\n",
    "\n",
    "model.save('spam_model.tfl')\n",
    "\n",
    "print(\"done training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry question std txt rate T C's apply\"]\n",
      "Ham: 0.00\n",
      "Spam: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Test predictions\n",
    "\n",
    "testIdx = 2\n",
    "\n",
    "input_array = np.array(list(vocab_proc.fit_transform(np.array([dataset.v2[testIdx]]))))\n",
    "\n",
    "reverse = np.array(list(vocab_proc.reverse(input_array)))\n",
    "print(reverse)\n",
    "\n",
    "prediction = model.predict(input_array)\n",
    "\n",
    "print(\"Ham: {0:.2f}\".format(prediction[0][0]))\n",
    "print(\"Spam: {0:.2f}\".format(prediction[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = \"SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info\"\n",
    "user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2311  200 2048  177  205  218    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n",
      "[ 'WIN CASH TODAY FREE pounds info <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>']\n",
      "Ham: 0.00\n",
      "Spam: 1.00\n"
     ]
    }
   ],
   "source": [
    "user_input = \"WIN CASH TODAY FREE pounds info\"\n",
    "\n",
    "# Transform manual input into vocabulary numbers\n",
    "input_array = np.array(list(vocab_proc.fit_transform(np.array([user_input]))))\n",
    "\n",
    "# reverse to check if the input matches what the dictionary has\n",
    "reverse = np.array(list(vocab_proc.reverse(input_array)))\n",
    "print(input_array)\n",
    "print(reverse)\n",
    "\n",
    "# perform prediction\n",
    "prediction = model.predict(input_array)\n",
    "print(\"Ham: {0:.2f}\".format(prediction[0][0]))\n",
    "print(\"Spam: {0:.2f}\".format(prediction[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
