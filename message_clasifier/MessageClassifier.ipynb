{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2 Unnamed: 2  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
      "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
      "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
      "\n",
      "  Unnamed: 3 Unnamed: 4  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n"
     ]
    }
   ],
   "source": [
    "# Deep Learning\n",
    "# Let's build a spam clasifier\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values with empty strings\n",
    "dataset.fillna(value='', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v1' 'v2' 'Unnamed: 2' 'Unnamed: 3' 'Unnamed: 4']\n"
     ]
    }
   ],
   "source": [
    "# score phrase and title\n",
    "print(dataset.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the required columns for inputs and outputs\n",
    "totalX = dataset.v2 # SMS text\n",
    "totalY = dataset.v1 # spam or ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn\n",
    "from tflearn.data_utils import pad_sequences, VocabularyProcessor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Convert the strings in the input into integers corresponding to the dictionary positions\n",
    "# Data is automatically padded so we need to pad_sequences manually\n",
    "vocab_proc = VocabularyProcessor(30) # max document length\n",
    "totalX = np.array(list(vocab_proc.fit_transform(totalX)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,     2,     3, ...,     0,     0,     0],\n",
       "       [   21,    22,    23, ...,     0,     0,     0],\n",
       "       [   27,    28,     8, ...,    49,    50,    51],\n",
       "       ..., \n",
       "       [11153,   415,     8, ...,     0,     0,     0],\n",
       "       [  174,  3484,   286, ...,     0,     0,     0],\n",
       "       [ 4643,   660,  1641, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will have 2 classes in total for prediction, indices from 0 to 1\n",
    "vocab_proc2 = VocabularyProcessor(1)\n",
    "totalY = np.array(list(vocab_proc2.fit_transform(totalY))) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       ..., \n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       ..., \n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# here totalY is numbered dictionary entries (0, 1, ... to 10)\n",
    "totalY[5]\n",
    "# Convert the indices into 11 dimensional vectors\n",
    "totalY = to_categorical(totalY, 2)\n",
    "# here totalY is a binary matrix\n",
    "totalY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  330   118   555   111    63 10566    77   118   771  5257    81    63\n",
      "  5591    53   583   648   111    91  4389     0     0     0     0     0\n",
      "     0     0     0     0     0     0]\n",
      "[ 452  548 1348   33  150 5709    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "[ 1.  0.]\n",
      "[ 1.  0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Split into training and testing data\n",
    "trainX, testX, trainY, testY = train_test_split(totalX, totalY, test_size=0.1)\n",
    "\n",
    "print(trainX[0])\n",
    "print(testX[0])\n",
    "print(trainY[0])\n",
    "print(testY[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Network building\n",
    "# 15 words max, so 15 input data\n",
    "net = tflearn.input_data([None, 30])\n",
    "\n",
    "# dictionary has 20k words max\n",
    "# Turns positive integers (indexes) into dense vectors of fixed size.\n",
    "net = tflearn.embedding(net, input_dim=20000, output_dim=128)\n",
    "\n",
    "# Long Short Term Memory Recurrent Layer.\n",
    "# Each input would have a size of i15x128 and each of these 128\n",
    "# sized vectors are fed into the LSTM layer one at a time.\n",
    "# All the intermediate outputs are collected and then passed on to the second LSTM layer.\n",
    "net = tflearn.lstm(net, 128, dropout=0.8)\n",
    "\n",
    "# The output is then sent to a fully connected layer that would give us our final 2 classes\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "\n",
    "# We use the adam optimizer instead of standard SGD since it converges much faster\n",
    "net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1569  | total loss: \u001b[1m\u001b[32m0.00039\u001b[0m\u001b[0m | time: 12.106s\n",
      "| Adam | epoch: 010 | loss: 0.00039 - acc: 1.0000 -- iter: 4992/5014\n",
      "Training Step: 1570  | total loss: \u001b[1m\u001b[32m0.00040\u001b[0m\u001b[0m | time: 13.187s\n",
      "| Adam | epoch: 010 | loss: 0.00040 - acc: 1.0000 | val_loss: 0.09412 - val_acc: 0.9821 -- iter: 5014/5014\n",
      "--\n",
      "INFO:tensorflow:/Users/miquel/dev/github/deep-learning/spam_model.tfl is not in all_model_checkpoint_paths. Manually adding it.\n",
      "done training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "\n",
    "# uncomment to load saved model\n",
    "# model.load('spam_model.tfl')\n",
    "\n",
    "# comment to use saved model\n",
    "model.fit(trainX, trainY, validation_set=(testX, testY), show_metric=True, batch_size=32)\n",
    "\n",
    "model.save('spam_model.tfl')\n",
    "\n",
    "print(\"done training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info\n",
      "spam\n",
      "[ 0.  1.]\n",
      "[ 0.00839915  0.99160087]\n"
     ]
    }
   ],
   "source": [
    "# Test predictions\n",
    "\n",
    "testIdx = 11\n",
    "\n",
    "prediction = model.predict(np.reshape(trainX[testIdx], (-1, 15)))\n",
    "\n",
    "print(dataset.v2[testIdx])\n",
    "print(dataset.v1[testIdx])\n",
    "print(totalY[testIdx])\n",
    "print(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = \"SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info\"\n",
    "user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 481 8041 2311  200 2048  177  205  218    0    0    0    0    0    0\n",
      "     0]]\n",
      "[ 'Hello Bye WIN CASH TODAY FREE pounds info <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>']\n",
      "Ham: 0.14\n",
      "Spam: 0.86\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Hello Bye WIN CASH TODAY FREE pounds info ajd;asdasd\"\n",
    "\n",
    "# Transform manual input into vocabulary numbers\n",
    "input_array = np.array(list(vocab_proc.fit_transform(np.array([user_input]))))\n",
    "\n",
    "# reverse to check if the input matches what the dictionary has\n",
    "reverse = np.array(list(vocab_proc.reverse(input_array)))\n",
    "print(input_array)\n",
    "print(reverse)\n",
    "\n",
    "# perform prediction\n",
    "prediction = model.predict(input_array)\n",
    "print(\"Ham: {0:.2f}\".format(prediction[0][0]))\n",
    "print(\"Spam: {0:.2f}\".format(prediction[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
